{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db37846-7f54-4162-9286-6013afbd6995",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting sqlalchemy<2.0\n  Downloading SQLAlchemy-1.4.52-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 10.8 MB/s eta 0:00:00\nCollecting greenlet!=0.4.17\n  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 616.0/616.0 kB 15.2 MB/s eta 0:00:00\nInstalling collected packages: greenlet, sqlalchemy\nSuccessfully installed greenlet-3.0.3 sqlalchemy-1.4.52\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install 'sqlalchemy < 2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b045930-547b-4924-aeea-c288bc33bfb2",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting s3fs\n  Downloading s3fs-2024.2.0-py3-none-any.whl (28 kB)\nCollecting aiobotocore<3.0.0,>=2.5.4\n  Downloading aiobotocore-2.12.1-py3-none-any.whl (76 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.3/76.3 kB 4.3 MB/s eta 0:00:00\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /databricks/python3/lib/python3.10/site-packages (from s3fs) (3.8.4)\nCollecting fsspec==2024.2.0\n  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 kB 6.3 MB/s eta 0:00:00\nRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /databricks/python3/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.14.1)\nCollecting botocore<1.34.52,>=1.34.41\n  Downloading botocore-1.34.51-py3-none-any.whl (12.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/12.0 MB 33.8 MB/s eta 0:00:00\nCollecting aioitertools<1.0.0,>=0.5.1\n  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /databricks/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.2)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\nRequirement already satisfied: urllib3<2.1,>=1.25.4 in /databricks/python3/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.11)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (0.10.0)\nRequirement already satisfied: idna>=2.0 in /databricks/python3/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.3)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\nInstalling collected packages: fsspec, aioitertools, botocore, aiobotocore, s3fs\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2022.7.1\n    Not uninstalling fsspec at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-854e2c8e-cb45-42d6-bac1-d12a127a2bae\n    Can't uninstall 'fsspec'. No files were found to uninstall.\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.27.28\n    Not uninstalling botocore at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-854e2c8e-cb45-42d6-bac1-d12a127a2bae\n    Can't uninstall 'botocore'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\nboto3 1.24.28 requires botocore<1.28.0,>=1.27.28, but you have botocore 1.34.51 which is incompatible.\nSuccessfully installed aiobotocore-2.12.1 aioitertools-0.11.0 botocore-1.34.51 fsspec-2024.2.0 s3fs-2024.2.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install 's3fs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb4a93b-7a66-44c7-b130-914ea8dc07d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import timedelta\n",
    "from sqlalchemy import Table, MetaData \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import s3fs\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import s3fs\n",
    "import joblib\n",
    "import datetime\n",
    "import matplotlib.ticker as mtick\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, BooleanType, ArrayType, IntegerType\n",
    "from pyspark.sql import functions as F, Window\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "#blocked out my AWS credentials for security\n",
    "user = '****'\n",
    "password = '****'\n",
    "access_key = '****'\n",
    "secret_access_key = '****'\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_access_key)\n",
    "\n",
    "jdbcURL_staging = f\"jdbc:postgresql://fw-rs-staging-us-east-1.cynyvmgqjdf4.us-east-1.redshift.amazonaws.com:5439/fwdb?user={user}&password={password}\"\n",
    "\n",
    "jdbcURL_qa = f\"jdbc:postgresql://fw-rs-qa.cynyvmgqjdf4.us-east-1.redshift.amazonaws.com:5439/fwdb?user={user}&password={password}\"\n",
    "\n",
    "jdbcURL_prod = f\"jdbc:postgresql://flywheel.cynyvmgqjdf4.us-east-1.redshift.amazonaws.com:5439/fwdb?user={user}&password={password}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a496fdcd-d161-4d64-9fc4-378f900fc066",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_wm_sales_brand(start_date, end_date, client_name, country):\n",
    "  #country = country.upper()\n",
    "  wm_sql_script = '''\n",
    "WITH CTE AS (SELECT DISTINCT \n",
    "  B.name as business_unit, \n",
    "  lower(A.brand_segmentation) as brand, \n",
    "  A.walmart_campaign_id campaignid,\n",
    "  nvl(A.item_segmentation, 'noSku') sku, \n",
    "  A.targeting_type,\n",
    "  F.campaign_name, \n",
    "  E.code as country\n",
    "FROM walmart.vw_walmart_segmentation A\n",
    "JOIN ams.t_business_unit B on A.business_unit_id = B.id\n",
    "JOIN ams.t_client_region C on B.client_country_id = C.client_country_id\n",
    "JOIN ams.t_client_country D on C.client_country_id = D.id\n",
    "JOIN ams.t_country E on D.country_id = E.id\n",
    "JOIN walmart.dim_walmart_campaign F on A.walmart_campaign_id = F.id\n",
    "WHERE b.name = '{client_name}'\n",
    "AND lower(A.targeting_type) != 'auto'\n",
    "AND E.code = '{country}'\n",
    "AND F.campaign_name LIKE 'SP%'\n",
    "  )\n",
    "    SELECT\n",
    "    B.country, \n",
    "    B.business_unit, \n",
    "    B.sku, \n",
    "    B.brand, \n",
    "    B.targeting_type,\n",
    "    A.formatted_date as date, \n",
    "    cast(dateadd(d, cast(-date_part(dow, A.formatted_date) as int), A.formatted_date) as date) firstdayofweek,\n",
    "    A.biddedkeyword, \n",
    "    A.searchedkeyword,\n",
    "    SUM(A.advertisedskusales14days + A.otherskusales14days) sales, \n",
    "    SUM(A.adspend) spend, \n",
    "    A.campaignid\n",
    "    FROM walmart.searched_keyword_dly A \n",
    "      JOIN CTE B ON A.campaignid = B.campaignid\n",
    "    WHERE A.formatted_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "    GROUP BY A.formatted_date, B.country, B.business_unit, B.brand, B.sku, B.targeting_type, A.biddedkeyword, A.searchedkeyword, A.campaignid\n",
    "    '''.format(start_date = start_date, end_date = end_date, client_name = client_name, country = country)\n",
    "    \n",
    "  wm_sales = spark.read \\\n",
    "          .format(\"com.databricks.spark.redshift\") \\\n",
    "          .option(\"url\", jdbcURL_prod) \\\n",
    "          .option(\"query\", wm_sql_script) \\\n",
    "          .option(\"tempdir\", \"s3a://fw-advanced-analytics/Zac/tmp\") \\\n",
    "          .option(\"forward_spark_s3_credentials\", \"true\") \\\n",
    "          .load()\n",
    "\n",
    "  \n",
    "    \n",
    "  return wm_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a528c284-91e2-46cb-911f-07eb8d7452c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_median_daily_rank_per_keyword(start_date, end_date, country, kw_list_sql):\n",
    "  from datetime import datetime, timedelta\n",
    "  orig_start_date = start_date\n",
    "  orig_end_date = end_date\n",
    "  orig_start_date = start_date + ' 00:00:00.000000'\n",
    "  orig_end_date = end_date + ' 23:59:59.000000'\n",
    "  \n",
    "  start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "  end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "# Subtract one day from start_date and add one day to end_date\n",
    "  new_start_date = start_date_obj - timedelta(days=1)\n",
    "  new_end_date = end_date_obj + timedelta(days=1)\n",
    "\n",
    "# Convert back to strings\n",
    "  start_date = new_start_date.strftime('%Y-%m-%d')\n",
    "  end_date = new_end_date.strftime('%Y-%m-%d')\n",
    "  \n",
    "  start_date = start_date + ' 00:00:00.000000'\n",
    "  end_date = end_date + ' 23:59:59.000000'\n",
    "\n",
    "    \n",
    "  AWS_ACCESS_KEY=\"****\"\n",
    "  AWS_SECRET_KEY=\"******\"\n",
    "  AWS_SESSION_TOKEN=\"*****\"\n",
    "  AWS_REGION = 'us-east-1'\n",
    "  SCHEMA_NAME = 'AwsDataCatalog'\n",
    "  S3_STAGING_DIR = 's3://athena-dc-datalake-query-results/zac/tmp'\n",
    "\n",
    "    # Initialize Athena client\n",
    "  athena_client = boto3.client(\n",
    "          \"athena\",\n",
    "          aws_access_key_id=AWS_ACCESS_KEY,\n",
    "          aws_secret_access_key=AWS_SECRET_KEY,\n",
    "          region_name=AWS_REGION, \n",
    "          aws_session_token = AWS_SESSION_TOKEN\n",
    "      )\n",
    "    #countries = ('US', 'CA')\n",
    "    \n",
    "  query_string = \"\"\"\n",
    "WITH CTE AS (\n",
    "    SELECT\n",
    "        A.asin_sku as sku,\n",
    "        A.keyword,\n",
    "        (A.utctime AT TIME ZONE 'UTC' AT TIME ZONE 'America/Los_Angeles') as pdttime,\n",
    "        A.absolute_rank as rank\n",
    "    FROM \"prd-silver\".sov_data A\n",
    "    WHERE\n",
    "        A.country = '{country}'\n",
    "        AND A.retailer = 'Walmart.com'\n",
    "        AND A.utctime BETWEEN CAST('{start_date}' AS TIMESTAMP) AND CAST('{end_date}' AS TIMESTAMP)\n",
    "        AND A.placement = 'Organic'\n",
    "        AND A.keyword in ({kw_list_sql})\n",
    ")\n",
    "    SELECT \n",
    "        sku, \n",
    "        keyword,\n",
    "        pdttime,\n",
    "        dense_rank() over (partition by keyword, pdttime order by rank asc) as rank \n",
    "    FROM CTE\n",
    "    WHERE pdttime BETWEEN CAST('{orig_start_date}' AS TIMESTAMP) AND CAST('{orig_end_date}' AS TIMESTAMP)\n",
    "\n",
    "  \"\"\".format(start_date=start_date, end_date=end_date, country=country, orig_start_date = orig_start_date, orig_end_date = orig_end_date, kw_list_sql = kw_list_sql)\n",
    "    \n",
    "\n",
    "  # Run the query and get the results\n",
    "  query_response = athena_client.start_query_execution(\n",
    "      QueryString=query_string,\n",
    "      QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "      ResultConfiguration={\n",
    "          \"OutputLocation\": \"s3://datadrop-test/fw_fish/tmp/\",\n",
    "          \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "      }\n",
    "  )\n",
    "\n",
    "  query_execution_id = query_response[\"QueryExecutionId\"]\n",
    "\n",
    "  # Check query execution status\n",
    "  query_execution = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "  query_execution_status = query_execution[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "\n",
    "  while query_execution_status in [\"QUEUED\", \"RUNNING\"]:\n",
    "      query_execution = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "      query_execution_status = query_execution[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "          \n",
    "      if query_execution_status == \"FAILED\":\n",
    "          error_message = query_execution[\"QueryExecution\"][\"Status\"][\"StateChangeReason\"]\n",
    "          raise Exception(f\"Athena query failed: {error_message}\")\n",
    "\n",
    "  # Initialize the next_token variable\n",
    "  next_token = None\n",
    "\n",
    "  # Initialize an empty list to store all rows\n",
    "  all_rows = []\n",
    "\n",
    "  # Retrieve the first set of results to get the header\n",
    "  first_query_results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "  header = [field[\"VarCharValue\"] for field in first_query_results[\"ResultSet\"][\"Rows\"][0][\"Data\"]]\n",
    "\n",
    "  # Extract rows from the first set of results\n",
    "  first_rows = [tuple(row[\"Data\"][i][\"VarCharValue\"] for i in range(len(row[\"Data\"]))) for row in first_query_results[\"ResultSet\"][\"Rows\"][1:]]\n",
    "  all_rows.extend(first_rows)\n",
    "\n",
    "  # Set next_token if available\n",
    "  next_token = first_query_results.get('NextToken', None)\n",
    "\n",
    "  # Loop to handle pagination for subsequent pages\n",
    "  while next_token:\n",
    "      # Get the query results for the current page\n",
    "      query_results = athena_client.get_query_results(QueryExecutionId=query_execution_id, NextToken=next_token)\n",
    "\n",
    "      # Extract rows and extend the all_rows list\n",
    "      rows = [tuple(row[\"Data\"][i][\"VarCharValue\"] for i in range(len(row[\"Data\"]))) for row in query_results[\"ResultSet\"][\"Rows\"]]\n",
    "      all_rows.extend(rows)\n",
    "\n",
    "      # Update the next_token\n",
    "      next_token = query_results.get('NextToken', None)\n",
    "\n",
    "  # Create the Spark DataFrame\n",
    "  sov = spark.createDataFrame(all_rows, header)\n",
    "\n",
    "  # Persist the DataFrame\n",
    "  sov.persist()\n",
    "\n",
    "\n",
    "  sov = sov.withColumn('date', F.date_format(F.col('pdttime'), 'yyyy-MM-dd'))\\\n",
    "              .withColumn(\"dayofweek\", F.dayofweek(F.col('pdttime')))\\\n",
    "              .withColumn(\"hour\", F.hour(F.col('pdttime')))\n",
    "\n",
    "  sov = sov.withColumn(\"dayofweek2\", F.dayofweek(F.col(\"date\")) - 1)\n",
    "\n",
    "# Subtract the day of the week from the date to get the first day of the week (Sunday)\n",
    "  sov = sov.withColumn(\"firstdayofweek\", F.expr(\"date_sub(date, dayofweek2)\"))\n",
    "  sov = sov.drop(\"dayofweek2\")      \n",
    "      \n",
    "  sov.createOrReplaceTempView('sov_tbl')\n",
    "  n_scrapes_keyword = spark.sql('select keyword, date, count(distinct pdttime) as n_scrapes from sov_tbl group by keyword, date order by date')\n",
    "  n_scrapes_keyword.persist()\n",
    "      \n",
    "  def hourly_weight_us():\n",
    "          df = pd.read_csv('/dbfs/mnt/fw-advanced-analytics/Fish/organic_search_impact/by_hour_of_week_data/Hershey.csv')\n",
    "          df = df.dropna(subset = ['event_hour_utc'])\n",
    "          df['event_hour_utc'] = df['event_hour_utc'].astype(int)\n",
    "          df['utc_time'] = df['event_date_utc'] + ' ' + df['event_hour_utc'].astype(str) + ':00:00'\n",
    "          df['utc_time'] = pd.to_datetime(df['utc_time'], format = '%Y-%m-%d %H:%M:%S')\n",
    "          df = spark.createDataFrame(df)\n",
    "          df = df.withColumn('pdt_time', F.from_utc_timestamp(df['utc_time'], 'PST'))\n",
    "          df = df.withColumn(\"dayofweek\", F.dayofweek(F.col(\"pdt_time\")))\\\n",
    "            .withColumn(\"hour\", F.hour(F.col(\"pdt_time\")))\n",
    "\n",
    "          by_hour_of_week = df.groupby(['dayofweek', 'hour']).agg(F.sum('product_sales').alias('product_sales'), F.sum('clicks').alias('clicks'))\n",
    "          by_hour_of_week = by_hour_of_week.withColumn('product_sales_by_clicks', by_hour_of_week.product_sales/by_hour_of_week.clicks)\n",
    "          by_hour_of_week = by_hour_of_week.withColumn('weight', (by_hour_of_week.agg(F.max('product_sales_by_clicks')).collect()[0][0])/by_hour_of_week.product_sales_by_clicks)\n",
    "          return by_hour_of_week.select(['dayofweek', 'hour', 'weight'])\n",
    "      \n",
    "      ####\n",
    "      ## Multiplying Factor for Hershey: US\n",
    "      ####\n",
    "      \n",
    "  by_hour_of_week = hourly_weight_us()\n",
    "      \n",
    "  def find_median(values_list):\n",
    "      try:\n",
    "          median = np.nanmedian(values_list) #get the median of values in a list in each row\n",
    "          return round(float(median), 2)\n",
    "      except Exception:\n",
    "          return None       # if there is anything wrong with the given values\n",
    "\n",
    "  median_finder = F.udf(find_median,T.FloatType())\n",
    "\n",
    "  sov_weight = sov.join(by_hour_of_week, on = ['dayofweek', 'hour'], how = 'left')\n",
    "  sov_weight = sov_weight.withColumn('rank_rescaled', sov_weight.rank * sov_weight.weight)\n",
    "\n",
    "  median_rank = sov_weight.groupBy(['keyword', 'sku', 'date', 'firstdayofweek']).agg(F.collect_list(\"rank_rescaled\").alias(\"rescaled_ranks\"))\n",
    "  median_rank = median_rank.withColumn(\"median_rank_rescaled\", median_finder(\"rescaled_ranks\"))\n",
    "\n",
    "  median_rank = median_rank.withColumn('date',  F.to_timestamp(F.col('date'), 'yyyy-MM-dd'))\n",
    "\n",
    "  median_rank_joined = median_rank.join(n_scrapes_keyword, on = ['keyword','date'], how = 'left')\n",
    "  daily_observation = sov.groupBy(['sku', 'date', 'keyword']).count()\n",
    "  daily_observation = daily_observation.withColumnRenamed('count', 'n_observations')\n",
    "  median_rank_joined2 = median_rank_joined.join(daily_observation, on = ['sku', 'date', 'keyword'], how = 'inner')\n",
    "  median_rank_joined2 = median_rank_joined2.withColumn('final_rank', F.sqrt(median_rank_joined2.n_scrapes/median_rank_joined2.n_observations) * median_rank_joined2.median_rank_rescaled)\n",
    "      \n",
    "  median_rank_joined2.createOrReplaceTempView('sov_tb2')\n",
    "      \n",
    "  final_daily_rank = spark.sql(\"\"\"\n",
    "                          select * \n",
    "                          from (\n",
    "                          select *, \n",
    "                          rank() over (partition by date, keyword order by final_rank asc) as adjusted_rank\n",
    "                          from sov_tb2 \n",
    "                          )\n",
    "                          order by date, keyword, adjusted_rank\n",
    "                          \"\"\")\n",
    "                          \n",
    "  final_daily_rank = final_daily_rank.select(['sku','date', 'keyword', 'firstdayofweek', 'adjusted_rank'])\n",
    "    #final_daily_rank = final_daily_rank.withColumn('date', F.date_format(F.col('date'), 'yyyy-MM-dd'))\n",
    "  return final_daily_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136efc21-0b60-4b17-9fdd-065cb2669351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_weekly_clickshare_brand(start_date, end_date, client_name,  country, department):\n",
    "  country = country.upper()\n",
    "  click_share_sql_script =  \"\"\"\n",
    "WITH CTE AS (\n",
    "  SELECT DISTINCT \n",
    "  B.name as business_unit, \n",
    "  lower(A.brand_segmentation) as brand, \n",
    "  A.walmart_campaign_id campaignid,\n",
    "  nvl(A.item_segmentation, 'noSku') sku, \n",
    "  A.targeting_type,\n",
    "  F.campaign_name, \n",
    "  E.code as country\n",
    "FROM walmart.vw_walmart_segmentation A\n",
    "JOIN ams.t_business_unit B on A.business_unit_id = B.id\n",
    "JOIN ams.t_client_region C on B.client_country_id = C.client_country_id\n",
    "JOIN ams.t_client_country D on C.client_country_id = D.id\n",
    "JOIN ams.t_country E on D.country_id = E.id\n",
    "JOIN walmart.dim_walmart_campaign F on A.walmart_campaign_id = F.id\n",
    "WHERE b.name = '{client_name}'\n",
    "AND E.code = '{country}'\n",
    "),\n",
    "keywords_cte as (\n",
    "SELECT \n",
    "  distinct A.searchedkeyword as keyword\n",
    "FROM walmart.searched_keyword_dly A\n",
    "JOIN CTE B ON A.campaignid = B.campaignid\n",
    "WHERE A.formatted_date between '{start_date}' and '{end_date}'\n",
    ")\n",
    "select \n",
    "  search_term  keyword,\n",
    "  start_date firstdayofweek,\n",
    "  ROUND(2*(no_one_click_share + no_two_click_share + no_three_click_share), 0)/2.0 as top_three_click_share\n",
    "FROM vc.fact_search_terms_weekly\n",
    "WHERE search_term in (select keyword from keywords_cte)\n",
    "AND start_date >= (select dateadd(d, cast(-date_part(dow,'{start_date}') as int), '{start_date}'))\n",
    "AND start_date <= '{end_date}'\n",
    "AND region = '{country}'\n",
    "AND interval = 'Weekly'\n",
    "AND department = '{department}'\n",
    "             \"\"\".format(start_date = start_date, \n",
    "                        end_date = end_date, \n",
    "                        country = country,\n",
    "                        client_name = client_name,\n",
    "                        department = 'Amazon.com'\n",
    "                        )\n",
    "    \n",
    "  click_share = spark.read \\\n",
    "          .format(\"com.databricks.spark.redshift\") \\\n",
    "          .option(\"url\", jdbcURL_prod) \\\n",
    "          .option(\"query\", click_share_sql_script) \\\n",
    "          .option(\"tempdir\", \"s3a://fw-advanced-analytics/Zac/tmp\") \\\n",
    "          .option(\"forward_spark_s3_credentials\", \"true\") \\\n",
    "          .load()\n",
    "  weights = pd.read_csv(\"/dbfs/mnt/fw-advanced-analytics/Fish/incrementality/clickshare_decay_100pct_95pct_90pct_first_page_exponential.csv\")\n",
    "  weights = weights[weights.first_page_clickshare == 100.0]\n",
    "  weights = weights[['top3_clickshare', 'decay']]\n",
    "  weights = spark.createDataFrame(weights)\n",
    "  cond = [weights.top3_clickshare == click_share.top_three_click_share]\n",
    "  joined = click_share.join(F.broadcast(weights), on = cond, how = 'inner')\n",
    "  return joined.select('keyword', 'firstdayofweek', 'top_three_click_share', 'decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50139999-331f-47c7-9de2-d50e9efad2b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_conversion_brand(start_date, end_date, client_name, country):\n",
    "  conversion_sql_script = \"\"\"\n",
    "WITH CTE AS (SELECT DISTINCT \n",
    "  B.name as business_unit, \n",
    "  lower(A.brand_segmentation) as brand, \n",
    "  A.walmart_campaign_id campaignid,\n",
    "  nvl(A.item_segmentation, 'noSku') sku, \n",
    "  A.targeting_type,\n",
    "  F.campaign_name, \n",
    "  E.code as country\n",
    "FROM walmart.vw_walmart_segmentation A\n",
    "JOIN ams.t_business_unit B on A.business_unit_id = B.id\n",
    "JOIN ams.t_client_region C on B.client_country_id = C.client_country_id\n",
    "JOIN ams.t_client_country D on C.client_country_id = D.id\n",
    "JOIN ams.t_country E on D.country_id = E.id\n",
    "JOIN walmart.dim_walmart_campaign F on A.walmart_campaign_id = F.id\n",
    "WHERE b.name = '{client_name}'\n",
    "AND lower(A.targeting_type) != 'auto'\n",
    "AND E.code = '{country}'\n",
    "AND F.campaign_name LIKE 'SP%'\n",
    "  )\n",
    "    SELECT \n",
    "    B.sku,\n",
    "    A.formatted_date as date,\n",
    "    cast(dateadd(d, cast(-date_part(dow, A.formatted_date) as int), A.formatted_date) as date) firstdayofweek,\n",
    "    SUM(A.unitssold14d) units_sold,\n",
    "    SUM(A.numadsclicks) clicks, \n",
    "    SUM(A.advertisedskusales14days + A.otherskusales14days) sales\n",
    "    FROM walmart.searched_keyword_dly A\n",
    "      JOIN CTE B ON A.campaignid = B.campaignid\n",
    "    WHERE A.formatted_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "    GROUP BY date, B.sku\n",
    "             \"\"\".format(client_name = client_name, \n",
    "                        country = country, \n",
    "                        start_date = start_date, \n",
    "                        end_date = end_date)\n",
    "  df = spark.read \\\n",
    "          .format(\"com.databricks.spark.redshift\") \\\n",
    "          .option(\"url\", jdbcURL_prod) \\\n",
    "          .option(\"query\", conversion_sql_script) \\\n",
    "          .option(\"tempdir\", \"s3a://fw-advanced-analytics/Zac/tmp\") \\\n",
    "          .option(\"forward_spark_s3_credentials\", \"true\") \\\n",
    "          .load()\n",
    "  \n",
    "  #df = df.groupBy(\"client_name\", \"country\", \"business_unit\", \"sku\", \"brand\", \"firstdayofweek\", \"date\", \"keyword\").agg(sum(\"units_sold\").alias(\"units_sold\"), sum(\"clicks\").alias(\"clicks\"))\n",
    "  df = df.filter(df['clicks'] > 0)\n",
    "\n",
    "  df.createOrReplaceTempView('tmp')\n",
    "  df = spark.sql(\"\"\"\n",
    "                 select sku, \n",
    "                        firstdayofweek, \n",
    "                        date,\n",
    "                        cast(units_sold as float)/cast(clicks as float) as conversion,\n",
    "                        cast(sales as float)/cast(units_sold as float) as asp\n",
    "                 from tmp\n",
    "                 \"\"\")\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c842b746-24de-4e6d-b9b5-851d432e9743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_incremental_sales(busisness_unit): \n",
    "  from pyspark.sql.functions import when\n",
    "\n",
    "  m_start = '2023-12-31'\n",
    "  m_end = '2024-04-09'\n",
    "  #bu = \"'Danone','Hersheys - US','Mattel - US', 'McCormick', 'Nestle - Purina', 'Nestle Health Science', 'Samsung TV', 'WhyteSpyder Bayer US', 'Clorox', 'Haleon-Consumer-Healthcare US', 'Revlon'\"\n",
    "  bu = business_unit\n",
    "  wm_sales_all = get_wm_sales_brand(m_start, m_end, bu ,'US')\n",
    "  wm_sales_all.persist()\n",
    "\n",
    "  wm_sales_all = wm_sales_all.withColumn(\n",
    "      \"brand\",\n",
    "      F.when(\n",
    "          (F.col(\"brand\").isNull()) & (F.col(\"business_unit\") == \"Samsung TV\"),\n",
    "          F.lit(\"samsung\")\n",
    "      ).otherwise(F.col(\"brand\"))\n",
    "  )\n",
    "  wm_sales_all = wm_sales_all.withColumn(\n",
    "      \"brand\",\n",
    "      F.when(\n",
    "          (F.col(\"brand\").isNull()) & (F.col(\"business_unit\") == \"WhyteSpyder Bayer US\"),\n",
    "          F.lit(\"bayer\")\n",
    "      ).otherwise(F.col(\"brand\"))\n",
    "  )\n",
    "\n",
    "  weekly_clickshare = get_weekly_clickshare_brand(m_start, m_end, bu, 'US', 'Amazon.com')\n",
    "  weekly_clickshare.persist()\n",
    "\n",
    "  #Prioritize searchedkeyword, use bidded if searched does not exist in clickshare table\n",
    "  wm_sales_all = wm_sales_all.alias(\"wm\")\n",
    "  clickshare = weekly_clickshare.alias(\"cs\")\n",
    "  # Perform a left join between wm_sales_all and clickshare\n",
    "  joined_df = wm_sales_all.join(\n",
    "      clickshare,\n",
    "      (F.col(\"wm.firstdayofweek\") == F.col(\"cs.firstdayofweek\")) & \n",
    "      (F.col(\"wm.searchedkeyword\") == F.col(\"cs.keyword\")),\n",
    "      'left'\n",
    "  )\n",
    "  # Create a new column 'keyword' in wm_sales_all\n",
    "  # Use 'searchedkeyword' if there's a match, otherwise use 'biddedkeyword'\n",
    "  wm_sales_all = joined_df.withColumn(\n",
    "      \"keyword\",\n",
    "      F.when(F.col(\"cs.keyword\").isNotNull(), F.col(\"wm.searchedkeyword\"))\n",
    "      .otherwise(F.col(\"wm.biddedkeyword\"))\n",
    "  )\n",
    "  # Drop the 'searchedkeyword' and 'biddedkeyword' columns\n",
    "  wm_sales_all = wm_sales_all.drop(\"searchedkeyword\", \"biddedkeyword\", \"top_three_click_share\", \"decay\")\n",
    "  # Optionally, drop the extra columns from clickshare if they were joined\n",
    "  #wm_sales_all = wm_sales_all.drop(*[column for column in clickshare.columns if column != 'keyword' o column != 'firstdayofweek'])\n",
    "\n",
    "  wm_sales = wm_sales_all.join(weekly_clickshare,  on = ['keyword', 'firstdayofweek'], how = 'leftsemi') # if keyword does not have sfr, exclude its sales\n",
    "  wm_sales.persist()\n",
    "\n",
    "  unique_kws = wm_sales.select(\"keyword\").distinct()\n",
    "  unique_kws_list = [row.keyword for row in unique_kws.collect()]\n",
    "  unique_kws = tuple(unique_kws_list)\n",
    "  unique_kws = tuple(brand.replace(\"'\", \"''\") for brand in unique_kws)    \n",
    "  kw_list_sql = ', '.join(f\"'{kw}'\" for kw in unique_kws)\n",
    "  daily_rank = get_median_daily_rank_per_keyword(m_start, m_end, 'US', kw_list_sql)\n",
    "  daily_rank.persist()\n",
    "  #Add brands to daily_rank\n",
    "  sales_brands = wm_sales_all.createOrReplaceTempView('brands_tbl')\n",
    "  sales_brands = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT sku, brand\n",
    "    FROM brands_tbl\n",
    "  \"\"\")\n",
    "  from pyspark.sql.functions import col\n",
    "  sales_brands = sales_brands.filter((col(\"sku\") != 'SB/SBV') & col(\"sku\").isNotNull() & (col(\"sku\") != 'noSku')) #Do not include sales not associated with an SKU\n",
    "  daily_rank = daily_rank.join(sales_brands, on = [\"sku\"], how = \"left\")\n",
    "\n",
    "  conversion = get_conversion_brand(m_start, m_end, bu, 'US')\n",
    "  conversion.persist()\n",
    "\n",
    "  daily_rank_conversion = daily_rank.join(conversion, on = ['sku', 'date'], how = 'left') \n",
    "  daily_rank_conversion.persist()\n",
    "\n",
    "  rank_clickshare = daily_rank_conversion.join(weekly_clickshare, on = ['keyword', 'firstdayofweek'], how = 'left')\n",
    "  rank_clickshare = rank_clickshare.withColumn('distribution', F.pow(rank_clickshare.decay, rank_clickshare.adjusted_rank))\n",
    "  rank_clickshare.persist() #Wegihts steepness of clickshare curve into rank\n",
    "\n",
    "  rank_clickshare_copy = rank_clickshare.toDF(*[col + '2' for col in rank_clickshare.columns])\n",
    "  cond = [rank_clickshare.brand == rank_clickshare_copy.brand2, \n",
    "        rank_clickshare.keyword == rank_clickshare_copy.keyword2,\n",
    "        rank_clickshare.date == rank_clickshare_copy.date2]\n",
    "  rank_clickshare_joined = rank_clickshare.join(rank_clickshare_copy, on = cond, how = 'outer')\n",
    "  rank_clickshare_joined.persist()\n",
    "\n",
    "  rank_clickshare_joined.createOrReplaceTempView('rank_clickshare_tbl')\n",
    "  same_brand = spark.sql(\"\"\"\n",
    "    select keyword, sku, date, brand,  sum(distribution2 * weight) as same_brand_distribution\n",
    "        from (\n",
    "      select *, \n",
    "      case \n",
    "      when (conversion2 is not null and conversion is not null and asp is not null and asp2 is not null) and  ((asp2 * conversion2) < (asp * conversion)) then 0.8 \n",
    "      when ((conversion2 is null or conversion is null) and (asp is not null and asp2 is not null)) and  (asp2  > asp) then 0.8 \n",
    "      else 1.0 \n",
    "      end as weight\n",
    "      from rank_clickshare_tbl \n",
    "      where sku != sku2 and distribution <= distribution2\n",
    "        )\n",
    "      group by keyword, sku, date, brand\n",
    "      \"\"\")\n",
    "  same_brand.persist()\n",
    "\n",
    "  rank_clickshare2 = rank_clickshare.join(same_brand, on = ['keyword', 'sku', 'date', 'brand'], how = 'left')\n",
    "  rank_clickshare2 = rank_clickshare2.withColumn(\n",
    "      'same_brand_distribution_unscaled', \n",
    "      F.sum('distribution').over(\n",
    "        Window.partitionBy(['brand', 'keyword', 'date'])\n",
    "            .orderBy(F.desc('distribution'))\n",
    "            .rowsBetween(Window.unboundedPreceding, -1)\n",
    "      )\n",
    "      ).withColumn(\n",
    "      'different_brand_distribution', \n",
    "      F.sum('distribution').over(\n",
    "        Window.partitionBy(['keyword', 'date']).orderBy(F.desc('distribution'))\n",
    "            .rowsBetween(Window.unboundedPreceding, -1)\n",
    "      ) - F.coalesce(F.col('same_brand_distribution_unscaled'), F.lit(0))\n",
    "      )\n",
    "  rank_clickshare2.persist()\n",
    "  rank_clickshare2 = rank_clickshare2.withColumn('prob_different_brand', F.col('different_brand_distribution')/(F.col('different_brand_distribution') + F.col   ('same_brand_distribution'))).fillna(value = 1, subset = ['prob_different_brand'])\n",
    "    \n",
    "  rank_clickshare2.createOrReplaceTempView('rank_clickshare_tbl2')\n",
    "  rank_clickshare3 = spark.sql(\"\"\"\n",
    "      select \n",
    "          keyword, \n",
    "          sku, \n",
    "          brand,\n",
    "          date, \n",
    "          adjusted_rank,\n",
    "          top_three_click_share, \n",
    "          decay, \n",
    "          distribution,\n",
    "          scaled_dist,\n",
    "          percent_total,\n",
    "          prob_different_brand,\n",
    "          pag1_different_brand_distribution\n",
    "      from (\n",
    "        select *,  (1.0 + ((distribution - min_d) * 9) / nullif((max_d - min_d), 0))/10.0   as scaled_dist,\n",
    "          nvl(max_different_brand_distribution/(max_different_brand_distribution + max_same_brand_distribution_unscaled), 1) as pag1_different_brand_distribution\n",
    "        from (\n",
    "        select *,  \n",
    "          max(same_brand_distribution_unscaled) over (PARTITION BY date, keyword, brand) as max_same_brand_distribution_unscaled,\n",
    "          max(different_brand_distribution) over (PARTITION BY date, keyword, brand) as max_different_brand_distribution,\n",
    "          max(distribution) over (PARTITION BY date, keyword) as max_d,\n",
    "          min(distribution) over (PARTITION BY date, keyword) as min_d,\n",
    "          distribution /nullif((sum(distribution) over (PARTITION BY date, keyword)), 0) as percent_total\n",
    "        from rank_clickshare_tbl2\n",
    "        )\n",
    "        )\n",
    "        order by percent_total desc nulls last\n",
    "      \"\"\")\n",
    "  rank_clickshare3.persist()\n",
    "  wm_sales_scrapped_keywords = wm_sales.join(rank_clickshare3, on = ['keyword', 'date'], how = 'leftsemi')\n",
    "  joined_df = wm_sales_scrapped_keywords.join(rank_clickshare3, on = ['keyword', 'date', 'sku', 'brand'], how = 'left')\n",
    "  rank_clickshare3_copy = rank_clickshare3.toDF(*[col + '2' for col in rank_clickshare3.columns])\\\n",
    "                                          .select('date2', 'brand2', 'keyword2', 'pag1_different_brand_distribution2').distinct()\n",
    "  rank_clickshare3_copy.persist()\n",
    "  cond = [joined_df.keyword == rank_clickshare3_copy.keyword2,\n",
    "              joined_df.date == rank_clickshare3_copy.date2,\n",
    "              joined_df.brand == rank_clickshare3_copy.brand2]\n",
    "  joined_df2 = joined_df.join(rank_clickshare3_copy, on = cond, how = 'left')\n",
    "  joined_df2.persist()\n",
    "\n",
    "  joined_df2.createOrReplaceTempView('rank_clickshare_tbl3')\n",
    "  incremental_sales = spark.sql(\"\"\"\n",
    "      select \n",
    "          keyword, \n",
    "          sku, \n",
    "          brand,\n",
    "          business_unit,\n",
    "          targeting_type,\n",
    "          date, \n",
    "          adjusted_rank,\n",
    "            top_three_click_share, \n",
    "            decay, \n",
    "            distribution,\n",
    "            scaled_dist,\n",
    "            percent_total,\n",
    "            prob_different_brand,\n",
    "            nvl(pag1_different_brand_distribution,1) as pag1_different_brand_distribution,\n",
    "            nvl(pag1_different_brand_distribution2,1) as pag1_different_brand_distribution2,\n",
    "          spend,\n",
    "          sales,\n",
    "          case when adjusted_rank <= 60 then sales * (1 - scaled_dist) * prob_different_brand\n",
    "            else sales * nvl(nvl(prob_different_brand, pag1_different_brand_distribution), \n",
    "                          nvl(pag1_different_brand_distribution2, 1)) end as incremental_sales,\n",
    "          case when adjusted_rank <= 60 then sales * (1 - scaled_dist) else sales end incremental_sales_older\n",
    "            \n",
    "      from  rank_clickshare_tbl3\n",
    "        order by incremental_sales desc\n",
    "      \"\"\")\n",
    "  return incremental_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3084f2-99ef-4d79-96fb-761ad46719bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[keyword: string, sku: string, brand: string, business_unit: string, targeting_type: string, date: date, adjusted_rank: int, top_three_click_share: decimal(16,4), decay: double, distribution: double, scaled_dist: double, percent_total: double, prob_different_brand: double, pag1_different_brand_distribution: double, pag1_different_brand_distribution2: double, spend: decimal(38,6), sales: decimal(38,6), incremental_sales: double, incremental_sales_older: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "m_start = '2023-08-01'\n",
    "m_end = '2024-02-29'\n",
    "#bu = \"'Danone','Hersheys - US','Mattel - US', 'McCormick', 'Nestle - Purina', 'Nestle Health Science', 'Samsung TV', 'WhyteSpyder Bayer US', 'Clorox', 'Haleon-Consumer-Healthcare US', 'Revlon'\"\n",
    "bu = 'SC Johnson'\n",
    "wm_sales_all = get_wm_sales_brand(m_start, m_end, bu ,'US')\n",
    "wm_sales_all.persist()\n",
    "\n",
    "wm_sales_all = wm_sales_all.withColumn(\n",
    "    \"brand\",\n",
    "    F.when(\n",
    "        (F.col(\"brand\").isNull()) & (F.col(\"business_unit\") == \"Samsung TV\"),\n",
    "        F.lit(\"samsung\")\n",
    "    ).otherwise(F.col(\"brand\"))\n",
    ")\n",
    "wm_sales_all = wm_sales_all.withColumn(\n",
    "    \"brand\",\n",
    "    F.when(\n",
    "        (F.col(\"brand\").isNull()) & (F.col(\"business_unit\") == \"WhyteSpyder Bayer US\"),\n",
    "        F.lit(\"bayer\")\n",
    "    ).otherwise(F.col(\"brand\"))\n",
    ")\n",
    "\n",
    "weekly_clickshare = get_weekly_clickshare_brand(m_start, m_end, bu, 'US', 'Amazon.com')\n",
    "weekly_clickshare.persist()\n",
    "\n",
    "#Prioritize searchedkeyword, use bidded if searched does not exist in clickshare table\n",
    "wm_sales_all = wm_sales_all.alias(\"wm\")\n",
    "clickshare = weekly_clickshare.alias(\"cs\")\n",
    "# Perform a left join between wm_sales_all and clickshare\n",
    "joined_df = wm_sales_all.join(\n",
    "    clickshare,\n",
    "    (F.col(\"wm.firstdayofweek\") == F.col(\"cs.firstdayofweek\")) & \n",
    "    (F.col(\"wm.searchedkeyword\") == F.col(\"cs.keyword\")),\n",
    "    'left'\n",
    ")\n",
    "# Create a new column 'keyword' in wm_sales_all\n",
    "# Use 'searchedkeyword' if there's a match, otherwise use 'biddedkeyword'\n",
    "wm_sales_all = joined_df.withColumn(\n",
    "    \"keyword\",\n",
    "    F.when(F.col(\"cs.keyword\").isNotNull(), F.col(\"wm.searchedkeyword\"))\n",
    "     .otherwise(F.col(\"wm.biddedkeyword\"))\n",
    ")\n",
    "# Drop the 'searchedkeyword' and 'biddedkeyword' columns\n",
    "wm_sales_all = wm_sales_all.drop(\"searchedkeyword\", \"biddedkeyword\", \"top_three_click_share\", \"decay\")\n",
    "# Optionally, drop the extra columns from clickshare if they were joined\n",
    "#wm_sales_all = wm_sales_all.drop(*[column for column in clickshare.columns if column != 'keyword' o column != 'firstdayofweek'])\n",
    "\n",
    "wm_sales = wm_sales_all.join(weekly_clickshare,  on = ['keyword', 'firstdayofweek'], how = 'leftsemi') # if keyword does not have sfr, exclude its sales\n",
    "wm_sales.persist()\n",
    "\n",
    "unique_kws = wm_sales.select(\"keyword\").distinct()\n",
    "unique_kws_list = [row.keyword for row in unique_kws.collect()]\n",
    "unique_kws = tuple(unique_kws_list)\n",
    "unique_kws = tuple(brand.replace(\"'\", \"''\") for brand in unique_kws)    \n",
    "kw_list_sql = ', '.join(f\"'{kw}'\" for kw in unique_kws)\n",
    "daily_rank = get_median_daily_rank_per_keyword(m_start, m_end, 'US', kw_list_sql)\n",
    "daily_rank.persist()\n",
    "#Add brands to daily_rank\n",
    "sales_brands = wm_sales_all.createOrReplaceTempView('brands_tbl')\n",
    "sales_brands = spark.sql(\"\"\"\n",
    "  SELECT DISTINCT sku, brand\n",
    "  FROM brands_tbl\n",
    "\"\"\")\n",
    "from pyspark.sql.functions import col\n",
    "sales_brands = sales_brands.filter((col(\"sku\") != 'SB/SBV') & col(\"sku\").isNotNull() & (col(\"sku\") != 'noSku')) #Do not include sales not associated with an SKU\n",
    "daily_rank = daily_rank.join(sales_brands, on = [\"sku\"], how = \"left\")\n",
    "\n",
    "conversion = get_conversion_brand(m_start, m_end, bu, 'US')\n",
    "conversion.persist()\n",
    "\n",
    "daily_rank_conversion = daily_rank.join(conversion, on = ['sku', 'date'], how = 'left') \n",
    "daily_rank_conversion.persist()\n",
    "\n",
    "rank_clickshare = daily_rank_conversion.join(weekly_clickshare, on = ['keyword', 'firstdayofweek'], how = 'left')\n",
    "rank_clickshare = rank_clickshare.withColumn('distribution', F.pow(rank_clickshare.decay, rank_clickshare.adjusted_rank))\n",
    "rank_clickshare.persist() #Wegihts steepness of clickshare curve into rank\n",
    "\n",
    "rank_clickshare_copy = rank_clickshare.toDF(*[col + '2' for col in rank_clickshare.columns])\n",
    "cond = [rank_clickshare.brand == rank_clickshare_copy.brand2, \n",
    "\t\t\trank_clickshare.keyword == rank_clickshare_copy.keyword2,\n",
    "\t\t\trank_clickshare.date == rank_clickshare_copy.date2]\n",
    "rank_clickshare_joined = rank_clickshare.join(rank_clickshare_copy, on = cond, how = 'outer')\n",
    "rank_clickshare_joined.persist()\n",
    "\n",
    "rank_clickshare_joined.createOrReplaceTempView('rank_clickshare_tbl')\n",
    "same_brand = spark.sql(\"\"\"\n",
    "\tselect keyword, sku, date, brand,  sum(distribution2 * weight) as same_brand_distribution\n",
    "\t\t   from (\n",
    "\t\tselect *, \n",
    "    case \n",
    "    when (conversion2 is not null and conversion is not null and asp is not null and asp2 is not null) and  ((asp2 * conversion2) < (asp * conversion)) then 0.8 \n",
    "\t\twhen ((conversion2 is null or conversion is null) and (asp is not null and asp2 is not null)) and  (asp2  > asp) then 0.8 \n",
    "     else 1.0 \n",
    "     end as weight\n",
    "\t\tfrom rank_clickshare_tbl \n",
    "\t\twhere sku != sku2 and distribution <= distribution2\n",
    "\t\t\t)\n",
    "\t\t group by keyword, sku, date, brand\n",
    "\t\t \"\"\")\n",
    "same_brand.persist()\n",
    "\n",
    "rank_clickshare2 = rank_clickshare.join(same_brand, on = ['keyword', 'sku', 'date', 'brand'], how = 'left')\n",
    "rank_clickshare2 = rank_clickshare2.withColumn(\n",
    "\t\t'same_brand_distribution_unscaled', \n",
    "\t\tF.sum('distribution').over(\n",
    "\t\t\tWindow.partitionBy(['brand', 'keyword', 'date'])\n",
    "\t\t\t\t  .orderBy(F.desc('distribution'))\n",
    "\t\t\t\t  .rowsBetween(Window.unboundedPreceding, -1)\n",
    "\t\t)\n",
    "\t\t).withColumn(\n",
    "\t\t'different_brand_distribution', \n",
    "\t\tF.sum('distribution').over(\n",
    "\t\t\tWindow.partitionBy(['keyword', 'date']).orderBy(F.desc('distribution'))\n",
    "\t\t\t\t  .rowsBetween(Window.unboundedPreceding, -1)\n",
    "\t\t) - F.coalesce(F.col('same_brand_distribution_unscaled'), F.lit(0))\n",
    "\t\t)\n",
    "rank_clickshare2.persist()\n",
    "rank_clickshare2 = rank_clickshare2.withColumn('prob_different_brand', F.col('different_brand_distribution')/(F.col('different_brand_distribution') + F.col   ('same_brand_distribution'))).fillna(value = 1, subset = ['prob_different_brand'])\n",
    "    \n",
    "rank_clickshare2.createOrReplaceTempView('rank_clickshare_tbl2')\n",
    "rank_clickshare3 = spark.sql(\"\"\"\n",
    "\t\tselect \n",
    "\t\t\t\tkeyword, \n",
    "\t\t\t\tsku, \n",
    "\t\t\t\tbrand,\n",
    "\t\t\t\tdate, \n",
    "\t\t\t\tadjusted_rank,\n",
    "\t\t\t\ttop_three_click_share, \n",
    "\t\t\t\tdecay, \n",
    "\t\t\t\tdistribution,\n",
    "\t\t\t\tscaled_dist,\n",
    "\t\t\t\tpercent_total,\n",
    "\t\t\t\tprob_different_brand,\n",
    "\t\t\t\tpag1_different_brand_distribution\n",
    "\t\tfrom (\n",
    "\t\t  select *,  (1.0 + ((distribution - min_d) * 9) / nullif((max_d - min_d), 0))/10.0   as scaled_dist,\n",
    "\t\t    nvl(max_different_brand_distribution/(max_different_brand_distribution + max_same_brand_distribution_unscaled), 1) as pag1_different_brand_distribution\n",
    "\t\t  from (\n",
    "\t\t\tselect *,  \n",
    "\t\t\t   max(same_brand_distribution_unscaled) over (PARTITION BY date, keyword, brand) as max_same_brand_distribution_unscaled,\n",
    "\t\t\t   max(different_brand_distribution) over (PARTITION BY date, keyword, brand) as max_different_brand_distribution,\n",
    "\t\t\t   max(distribution) over (PARTITION BY date, keyword) as max_d,\n",
    "\t\t\t   min(distribution) over (PARTITION BY date, keyword) as min_d,\n",
    "\t\t\t  distribution /nullif((sum(distribution) over (PARTITION BY date, keyword)), 0) as percent_total\n",
    "\t\t   from rank_clickshare_tbl2\n",
    "\t\t\t )\n",
    "\t\t   )\n",
    "\t\t  order by percent_total desc nulls last\n",
    "\t\t\"\"\")\n",
    "rank_clickshare3.persist()\n",
    "wm_sales_scrapped_keywords = wm_sales.join(rank_clickshare3, on = ['keyword', 'date'], how = 'leftsemi')\n",
    "joined_df = wm_sales_scrapped_keywords.join(rank_clickshare3, on = ['keyword', 'date', 'sku', 'brand'], how = 'left')\n",
    "rank_clickshare3_copy = rank_clickshare3.toDF(*[col + '2' for col in rank_clickshare3.columns])\\\n",
    "                                        .select('date2', 'brand2', 'keyword2', 'pag1_different_brand_distribution2').distinct()\n",
    "rank_clickshare3_copy.persist()\n",
    "cond = [joined_df.keyword == rank_clickshare3_copy.keyword2,\n",
    "            joined_df.date == rank_clickshare3_copy.date2,\n",
    "            joined_df.brand == rank_clickshare3_copy.brand2]\n",
    "joined_df2 = joined_df.join(rank_clickshare3_copy, on = cond, how = 'left')\n",
    "joined_df2.persist()\n",
    "\n",
    "joined_df2.createOrReplaceTempView('rank_clickshare_tbl3')\n",
    "incremental_sales = spark.sql(\"\"\"\n",
    "\t\tselect \n",
    "\t\t\t\tkeyword, \n",
    "\t\t\t\tsku, \n",
    "\t\t\t\tbrand,\n",
    "\t\t\t\tbusiness_unit,\n",
    "\t\t\t\ttargeting_type,\n",
    "\t\t\t\tdate, \n",
    "\t\t\t\tadjusted_rank,\n",
    "\t\t\t    top_three_click_share, \n",
    "\t\t\t    decay, \n",
    "\t\t\t    distribution,\n",
    "\t\t\t    scaled_dist,\n",
    "\t\t\t    percent_total,\n",
    "\t\t\t    prob_different_brand,\n",
    "\t\t\t    nvl(pag1_different_brand_distribution,1) as pag1_different_brand_distribution,\n",
    "\t\t\t    nvl(pag1_different_brand_distribution2,1) as pag1_different_brand_distribution2,\n",
    "\t\t\t\tspend,\n",
    "\t\t\t\tsales,\n",
    "\t\t\t\tcase when adjusted_rank <= 60 then sales * (1 - scaled_dist) * prob_different_brand\n",
    "\t\t\t\t  else sales * nvl(nvl(prob_different_brand, pag1_different_brand_distribution), \n",
    "\t\t\t\t                nvl(pag1_different_brand_distribution2, 1)) end as incremental_sales,\n",
    "\t\t\t\tcase when adjusted_rank <= 60 then sales * (1 - scaled_dist) else sales end incremental_sales_older\n",
    "\t\t\t\t  \n",
    "\t\tfrom  rank_clickshare_tbl3\n",
    "\t\t  order by incremental_sales desc\n",
    "\t\t\"\"\")\n",
    "incremental_sales.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a461884-c646-4a88-b7e8-122894a7ac68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:105: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [top_three_click_share, spend, sales] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add 'year_month' column to 'incremental_sales' DataFrame\n",
    "incremental_sales = incremental_sales.withColumn('year_month', F.date_format(F.col('date'), 'yyyy-MM'))\n",
    "\n",
    "# Get the distinct combinations of year-month and business units\n",
    "combo_list = incremental_sales.select('year_month', 'business_unit').distinct().collect()\n",
    "\n",
    "# Iterate over each combination of year-month and business unit\n",
    "for row in combo_list:\n",
    "    year_month = row.year_month\n",
    "    business_unit = row.business_unit\n",
    "\n",
    "    # Filter for the specific year-month and business unit\n",
    "    filtered_df = incremental_sales.filter(\n",
    "        (F.col('year_month') == year_month) & \n",
    "        (F.col('business_unit') == business_unit)\n",
    "    )\n",
    "\n",
    "    # Convert to Pandas DataFrame (ensure this is feasible size-wise)\n",
    "    pandas_df = filtered_df.toPandas()\n",
    "\n",
    "    # Define the file path, incorporating the business_unit dynamically\n",
    "    file_path = f\"/dbfs/mnt/fw-advanced-analytics/Zac/Walmart Incrementality Final Results/{business_unit}/{business_unit} {year_month}.csv\"\n",
    "\n",
    "    # Save to CSV\n",
    "    pandas_df.to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "WMI VFINAL by BU Looping",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
